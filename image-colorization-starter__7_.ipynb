{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2020-08-22T14:51:42.305967Z","iopub.status.busy":"2020-08-22T14:51:42.305043Z","iopub.status.idle":"2020-08-22T14:51:42.308017Z","shell.execute_reply":"2020-08-22T14:51:42.307452Z"},"papermill":{"duration":0.020028,"end_time":"2020-08-22T14:51:42.308128","exception":false,"start_time":"2020-08-22T14:51:42.288100","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n #   for filename in filenames:\n  #      print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.012052,"end_time":"2020-08-22T14:51:42.331550","exception":false,"start_time":"2020-08-22T14:51:42.319498","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Loading the Data\nImporting all the necessary libraries required to run the following code for image colourization."},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T14:51:42.359356Z","iopub.status.busy":"2020-08-22T14:51:42.358616Z","iopub.status.idle":"2020-08-22T14:51:44.334467Z","shell.execute_reply":"2020-08-22T14:51:44.333832Z"},"papermill":{"duration":1.992486,"end_time":"2020-08-22T14:51:44.334581","exception":false,"start_time":"2020-08-22T14:51:42.342095","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from torchvision.datasets import ImageFolder\nimport torchvision.transforms as T\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.dataset import random_split\nimport torch.nn.functional as F\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom skimage.color import rgb2lab, lab2rgb, rgb2gray","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.010894,"end_time":"2020-08-22T14:51:44.359163","exception":false,"start_time":"2020-08-22T14:51:44.348269","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Defining the directory for the dataset to be used. The dataset used here is a small 6 GB snippet of the ImageNet dataset. It has 2 classes: 'train' and 'val', containing 45000 and 5000 images respectively."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2020-08-22T14:51:44.385920Z","iopub.status.busy":"2020-08-22T14:51:44.385039Z","iopub.status.idle":"2020-08-22T14:51:44.388029Z","shell.execute_reply":"2020-08-22T14:51:44.387536Z"},"papermill":{"duration":0.017752,"end_time":"2020-08-22T14:51:44.388127","exception":false,"start_time":"2020-08-22T14:51:44.370375","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"DATA_DIR = '../input/imagenet/imagenet/'","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.010743,"end_time":"2020-08-22T14:51:44.409490","exception":false,"start_time":"2020-08-22T14:51:44.398747","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Loading images contained in the data directory into a avriable called dataset using ImageFolder, and applying two transforms to all the images:\n1. Resizing the non-uniformly sized images into 256x256 images.\n2. Converting them into tensors."},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T14:51:44.435945Z","iopub.status.busy":"2020-08-22T14:51:44.435379Z","iopub.status.idle":"2020-08-22T14:52:08.927903Z","shell.execute_reply":"2020-08-22T14:52:08.927264Z"},"papermill":{"duration":24.507303,"end_time":"2020-08-22T14:52:08.928018","exception":false,"start_time":"2020-08-22T14:51:44.420715","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"dataset = ImageFolder(DATA_DIR, transform=T.Compose([T.Resize((256, 256)),T.ToTensor()]))","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.012059,"end_time":"2020-08-22T14:52:08.951255","exception":false,"start_time":"2020-08-22T14:52:08.939196","status":"completed"},"tags":[]},"cell_type":"markdown","source":"As mentioned above, the dataset has a total 50000 images."},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T14:52:08.980831Z","iopub.status.busy":"2020-08-22T14:52:08.980218Z","iopub.status.idle":"2020-08-22T14:52:08.986649Z","shell.execute_reply":"2020-08-22T14:52:08.986079Z"},"papermill":{"duration":0.02362,"end_time":"2020-08-22T14:52:08.986751","exception":false,"start_time":"2020-08-22T14:52:08.963131","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"len(dataset)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.011414,"end_time":"2020-08-22T14:52:09.009364","exception":false,"start_time":"2020-08-22T14:52:08.997950","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Creating the training and validation sets. A seed 42 is set to have the same training and validation datasets each time the notebook is run, and the two datasets are split using the random_split function from the pytorch libraries."},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T14:52:09.038022Z","iopub.status.busy":"2020-08-22T14:52:09.037280Z","iopub.status.idle":"2020-08-22T14:52:09.058363Z","shell.execute_reply":"2020-08-22T14:52:09.058918Z"},"papermill":{"duration":0.038485,"end_time":"2020-08-22T14:52:09.059032","exception":false,"start_time":"2020-08-22T14:52:09.020547","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"random_seed = 42\ntorch.manual_seed(random_seed)\n\nval_size = 1000\ntrain_size = len(dataset) - val_size\n\ntrain_ds, val_ds = random_split(dataset, [train_size, val_size])\nlen(train_ds), len(val_ds)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.010927,"end_time":"2020-08-22T14:52:09.081630","exception":false,"start_time":"2020-08-22T14:52:09.070703","status":"completed"},"tags":[]},"cell_type":"markdown","source":"The batch size is set to be 128."},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T14:52:09.109262Z","iopub.status.busy":"2020-08-22T14:52:09.108484Z","iopub.status.idle":"2020-08-22T14:52:09.111503Z","shell.execute_reply":"2020-08-22T14:52:09.110936Z"},"papermill":{"duration":0.018277,"end_time":"2020-08-22T14:52:09.111589","exception":false,"start_time":"2020-08-22T14:52:09.093312","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"batch_size = 128","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.01183,"end_time":"2020-08-22T14:52:09.134455","exception":false,"start_time":"2020-08-22T14:52:09.122625","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Loading the training and validation datasets into the CPU using DataLoader."},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T14:52:09.163251Z","iopub.status.busy":"2020-08-22T14:52:09.162390Z","iopub.status.idle":"2020-08-22T14:52:09.165560Z","shell.execute_reply":"2020-08-22T14:52:09.165003Z"},"papermill":{"duration":0.019668,"end_time":"2020-08-22T14:52:09.165690","exception":false,"start_time":"2020-08-22T14:52:09.146022","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_loader = DataLoader(train_ds, batch_size = batch_size, shuffle = True, num_workers = 4, pin_memory = True)\nval_loader = DataLoader(val_ds, batch_size = batch_size, num_workers = 4, pin_memory = True)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.011612,"end_time":"2020-08-22T14:52:09.188933","exception":false,"start_time":"2020-08-22T14:52:09.177321","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Make cuda(GPU) the device if availability permits."},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T14:52:09.565583Z","iopub.status.busy":"2020-08-22T14:52:09.562427Z","iopub.status.idle":"2020-08-22T14:52:09.575078Z","shell.execute_reply":"2020-08-22T14:52:09.576033Z"},"papermill":{"duration":0.375682,"end_time":"2020-08-22T14:52:09.576253","exception":false,"start_time":"2020-08-22T14:52:09.200571","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ndevice","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.018322,"end_time":"2020-08-22T14:52:09.617444","exception":false,"start_time":"2020-08-22T14:52:09.599122","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Define a function to load data into the device assigned above."},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T14:52:09.658904Z","iopub.status.busy":"2020-08-22T14:52:09.657962Z","iopub.status.idle":"2020-08-22T14:52:09.661934Z","shell.execute_reply":"2020-08-22T14:52:09.662593Z"},"papermill":{"duration":0.029018,"end_time":"2020-08-22T14:52:09.662791","exception":false,"start_time":"2020-08-22T14:52:09.633773","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def to_device(data, device):\n    if isinstance(data, (list, tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking = True)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.018297,"end_time":"2020-08-22T14:52:09.699843","exception":false,"start_time":"2020-08-22T14:52:09.681546","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Define a class to load data into a GPU, if it's available, otherwise into a CPU."},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T14:52:09.744139Z","iopub.status.busy":"2020-08-22T14:52:09.743244Z","iopub.status.idle":"2020-08-22T14:52:09.748616Z","shell.execute_reply":"2020-08-22T14:52:09.749395Z"},"papermill":{"duration":0.032118,"end_time":"2020-08-22T14:52:09.749570","exception":false,"start_time":"2020-08-22T14:52:09.717452","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"class DeviceDataLoader():\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n\n    def __iter__(self):\n        for batch in self.dl:\n            yield to_device(batch, self.device)\n  \n    def __len__(self):\n        return len(self.dl)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.016033,"end_time":"2020-08-22T14:52:09.782630","exception":false,"start_time":"2020-08-22T14:52:09.766597","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Since we are using a GPU, load the training and validation datasets into cuda(the selected device)."},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T14:52:09.823892Z","iopub.status.busy":"2020-08-22T14:52:09.822993Z","iopub.status.idle":"2020-08-22T14:52:09.825538Z","shell.execute_reply":"2020-08-22T14:52:09.824699Z"},"papermill":{"duration":0.026386,"end_time":"2020-08-22T14:52:09.825686","exception":false,"start_time":"2020-08-22T14:52:09.799300","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_loader = DeviceDataLoader(train_loader, device)\nval_loader = DeviceDataLoader(val_loader, device)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.017339,"end_time":"2020-08-22T14:52:09.860194","exception":false,"start_time":"2020-08-22T14:52:09.842855","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Defining the Model\nThe images are originally using the RGB colormodel. But for the purpose of image colourization, they have to be converted to Lab. The CIELAB is a colorspace which has 3 channels: L, a, and b. The function defined below converts images from RGB to CIELAB, and then splits the L and ab channels into the variables X and Y respectively. The range of ab channels is from -128 to 127, hence, the ab channels are normalized by dividing it by 128. The function returns finally returns X and Y loaded into cuda."},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T14:52:09.917852Z","iopub.status.busy":"2020-08-22T14:52:09.912950Z","iopub.status.idle":"2020-08-22T14:52:09.921144Z","shell.execute_reply":"2020-08-22T14:52:09.922155Z"},"papermill":{"duration":0.040417,"end_time":"2020-08-22T14:52:09.922307","exception":false,"start_time":"2020-08-22T14:52:09.881890","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def generate_l_ab(images): \n    lab = rgb2lab(images.permute(0, 2, 3, 1).cpu().numpy())\n    X = lab[:,:,:,0]\n    X = X.reshape(X.shape+(1,))\n    Y = lab[:,:,:,1:] / 128\n    return to_device(torch.tensor(X, dtype = torch.float).permute(0, 3, 1, 2), device),to_device(torch.tensor(Y, dtype = torch.float).permute(0, 3, 1, 2), device)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.021123,"end_time":"2020-08-22T14:52:09.964680","exception":false,"start_time":"2020-08-22T14:52:09.943557","status":"completed"},"tags":[]},"cell_type":"markdown","source":"The Base Model class is defined as containing three functions. \n1. The *training_batch* function takes the batch of 128 images as input, generates the values of L and ab channels using the *generate_l_ab* function, gets the predicted ab channels using the forward function of the model, and calculates the MSE Loss between the actual and predicted ab channels.\n2. The *validation_batch* functions performs the same task as the *training_batch* function, except for the images in the validation dataset, which is evident from the function names.\n3. The function *validation_end_epoch* returns the average loss on the validation dataset."},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T14:52:10.031094Z","iopub.status.busy":"2020-08-22T14:52:10.030208Z","iopub.status.idle":"2020-08-22T14:52:10.033539Z","shell.execute_reply":"2020-08-22T14:52:10.034630Z"},"papermill":{"duration":0.044253,"end_time":"2020-08-22T14:52:10.034827","exception":false,"start_time":"2020-08-22T14:52:09.990574","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"class BaseModel(nn.Module):\n    def training_batch(self, batch):\n        images, _ = batch\n        X, Y = generate_l_ab(images)\n        outputs = self.forward(X)\n        loss = F.mse_loss(outputs, Y)\n        return loss\n\n    def validation_batch(self, batch):\n        images, _ = batch\n        X, Y = generate_l_ab(images)\n        outputs = self.forward(X)\n        loss = F.mse_loss(outputs, Y)\n        return {'val_loss' : loss.item()}\n\n    def validation_end_epoch(self, outputs):\n        epoch_loss = sum([x['val_loss'] for x in outputs]) / len(outputs)\n        return {'epoch_loss' : epoch_loss}","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.011923,"end_time":"2020-08-22T14:52:10.065361","exception":false,"start_time":"2020-08-22T14:52:10.053438","status":"completed"},"tags":[]},"cell_type":"markdown","source":"A helper function is defined to get the appropriate padding in order to keep the size of the output image same as the input image during convolution."},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T14:52:10.096542Z","iopub.status.busy":"2020-08-22T14:52:10.095673Z","iopub.status.idle":"2020-08-22T14:52:10.098400Z","shell.execute_reply":"2020-08-22T14:52:10.097833Z"},"papermill":{"duration":0.020615,"end_time":"2020-08-22T14:52:10.098496","exception":false,"start_time":"2020-08-22T14:52:10.077881","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def get_padding(kernel_size: int, stride: int = 1, dilation: int = 1, **_) -> int:\n    padding = ((stride - 1) + dilation * (kernel_size - 1)) // 2\n    return padding","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.013864,"end_time":"2020-08-22T14:52:10.125429","exception":false,"start_time":"2020-08-22T14:52:10.111565","status":"completed"},"tags":[]},"cell_type":"markdown","source":"The *Encoder_Decoder* class is an extension of the Base Model and it contains the code for the architecture of the model."},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T14:52:10.175519Z","iopub.status.busy":"2020-08-22T14:52:10.173876Z","iopub.status.idle":"2020-08-22T14:52:10.176320Z","shell.execute_reply":"2020-08-22T14:52:10.176795Z"},"papermill":{"duration":0.035342,"end_time":"2020-08-22T14:52:10.176909","exception":false,"start_time":"2020-08-22T14:52:10.141567","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"class Encoder_Decoder(BaseModel):\n    def __init__(self):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Conv2d(1, 64, kernel_size = 3, stride = 2, padding = get_padding(3, 2)),\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.Conv2d(64, 128, kernel_size = 3, padding = get_padding(3)),\n            nn.ReLU(),\n            nn.BatchNorm2d(128),\n        \n            nn.Conv2d(128, 128, kernel_size = 3, stride = 2, padding = get_padding(3, 2)),\n            nn.ReLU(),\n            nn.BatchNorm2d(128),\n            nn.Conv2d(128, 256, kernel_size = 3, padding = get_padding(3)),\n            nn.ReLU(),\n            nn.BatchNorm2d(256),\n        \n            nn.Conv2d(256, 256, kernel_size = 3, stride = 2, padding = get_padding(3, 2)),\n            nn.ReLU(),\n            nn.BatchNorm2d(256),\n            nn.Conv2d(256, 512, kernel_size = 3, padding = get_padding(3)),\n            nn.ReLU(),\n            nn.BatchNorm2d(512),\n            \n            nn.Conv2d(512, 512, kernel_size = 3, padding = get_padding(3)),\n            nn.ReLU(),\n            nn.BatchNorm2d(512),\n            nn.Conv2d(512, 256, kernel_size = 3, padding = get_padding(3)),\n            nn.ReLU(),\n            nn.BatchNorm2d(256),\n        \n            nn.Conv2d(256, 128, kernel_size = 3, padding = get_padding(3)),\n            nn.Upsample(size = (64,64)),\n            nn.Conv2d(128, 64, kernel_size = 3, padding = get_padding(3)),\n            nn.Upsample(size = (128,128)),\n            nn.Conv2d(64, 32, kernel_size = 3, padding = get_padding(3)),\n            nn.Conv2d(32, 16, kernel_size = 3, padding = get_padding(3)),\n            nn.Conv2d(16, 2, kernel_size = 3, padding = get_padding(3)),\n            nn.Tanh(),\n            nn.Upsample(size = (256,256))\n    )\n\n    def forward(self, images):\n        return self.network(images)     \n    ","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.012912,"end_time":"2020-08-22T14:52:10.202543","exception":false,"start_time":"2020-08-22T14:52:10.189631","status":"completed"},"tags":[]},"cell_type":"markdown","source":"The model is defined and loaded to cuda."},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T14:52:10.240062Z","iopub.status.busy":"2020-08-22T14:52:10.239453Z","iopub.status.idle":"2020-08-22T14:52:15.036870Z","shell.execute_reply":"2020-08-22T14:52:15.036375Z"},"papermill":{"duration":4.821285,"end_time":"2020-08-22T14:52:15.036987","exception":false,"start_time":"2020-08-22T14:52:10.215702","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"model = Encoder_Decoder()\nto_device(model, device)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.013795,"end_time":"2020-08-22T14:52:15.063864","exception":false,"start_time":"2020-08-22T14:52:15.050069","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Training the Model\nThe *validate* and *fit* functions are defined to keep track of the loss and train the model."},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T14:52:15.100048Z","iopub.status.busy":"2020-08-22T14:52:15.099264Z","iopub.status.idle":"2020-08-22T14:52:15.102265Z","shell.execute_reply":"2020-08-22T14:52:15.101658Z"},"papermill":{"duration":0.025963,"end_time":"2020-08-22T14:52:15.102359","exception":false,"start_time":"2020-08-22T14:52:15.076396","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"@torch.no_grad()\ndef validate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_batch(batch) for batch in val_loader]\n    return model.validation_end_epoch(outputs)\n\ndef fit(model, epochs, learning_rate, train_loader, val_loader, optimization_func = torch.optim.SGD):\n    torch.cuda.empty_cache()\n    history = []\n    optimizer = optimization_func(model.parameters(), learning_rate)\n    for epoch in range(epochs):\n        train_losses = []\n        model.train()\n        for batch in tqdm(train_loader):\n            loss = model.training_batch(batch)\n            train_losses.append(loss)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        result = validate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        history.append(result)\n        print('Epoch: {}, Train loss: {:.4f}, Validation loss: {:.4f}'.format(epoch, result['train_loss'], result['epoch_loss']))\n    return history","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.01214,"end_time":"2020-08-22T14:52:15.126970","exception":false,"start_time":"2020-08-22T14:52:15.114830","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Initializing the loss."},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T14:52:15.159202Z","iopub.status.busy":"2020-08-22T14:52:15.158560Z","iopub.status.idle":"2020-08-22T14:52:42.207122Z","shell.execute_reply":"2020-08-22T14:52:42.207823Z"},"papermill":{"duration":27.068739,"end_time":"2020-08-22T14:52:42.207982","exception":false,"start_time":"2020-08-22T14:52:15.139243","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"history = [validate(model, val_loader)]\nhistory","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.012347,"end_time":"2020-08-22T14:52:42.233243","exception":false,"start_time":"2020-08-22T14:52:42.220896","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Training."},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T14:52:42.291345Z","iopub.status.busy":"2020-08-22T14:52:42.290407Z","iopub.status.idle":"2020-08-22T18:02:13.963807Z","shell.execute_reply":"2020-08-22T18:02:13.963111Z"},"papermill":{"duration":11371.71817,"end_time":"2020-08-22T18:02:13.963976","exception":false,"start_time":"2020-08-22T14:52:42.245806","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"history += fit(model, 5, 0.001, train_loader, val_loader, torch.optim.Adam)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.013272,"end_time":"2020-08-22T18:02:13.992021","exception":false,"start_time":"2020-08-22T18:02:13.978749","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Saving the model to be able to continue training the model from the same point."},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T18:02:14.108609Z","iopub.status.busy":"2020-08-22T18:02:14.108041Z","iopub.status.idle":"2020-08-22T18:02:14.219070Z","shell.execute_reply":"2020-08-22T18:02:14.219673Z"},"papermill":{"duration":0.128762,"end_time":"2020-08-22T18:02:14.219871","exception":false,"start_time":"2020-08-22T18:02:14.091109","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"torch.save(model.state_dict(), 'test12.pth')","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.013698,"end_time":"2020-08-22T18:02:14.248083","exception":false,"start_time":"2020-08-22T18:02:14.234385","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Loading the pth file to continue training from the same parameters onwards."},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T18:02:14.281267Z","iopub.status.busy":"2020-08-22T18:02:14.280346Z","iopub.status.idle":"2020-08-22T18:02:14.283752Z","shell.execute_reply":"2020-08-22T18:02:14.284201Z"},"papermill":{"duration":0.02263,"end_time":"2020-08-22T18:02:14.284324","exception":false,"start_time":"2020-08-22T18:02:14.261694","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def load_checkpoint(filepath): \n    model = Encoder_Decoder()\n    model.load_state_dict(torch.load(filepath))\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T18:02:14.315281Z","iopub.status.busy":"2020-08-22T18:02:14.314669Z","iopub.status.idle":"2020-08-22T18:02:14.430560Z","shell.execute_reply":"2020-08-22T18:02:14.431179Z"},"papermill":{"duration":0.13343,"end_time":"2020-08-22T18:02:14.431324","exception":false,"start_time":"2020-08-22T18:02:14.297894","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"model = load_checkpoint('./test12.pth')\nto_device(model, device)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.013727,"end_time":"2020-08-22T18:02:14.459531","exception":false,"start_time":"2020-08-22T18:02:14.445804","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Testing the Model\nCombining the L channel and the predicted ab channels and converting it to RGB to obtain the final colour image."},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T18:02:14.496346Z","iopub.status.busy":"2020-08-22T18:02:14.495003Z","iopub.status.idle":"2020-08-22T18:02:14.497577Z","shell.execute_reply":"2020-08-22T18:02:14.498060Z"},"papermill":{"duration":0.024662,"end_time":"2020-08-22T18:02:14.498174","exception":false,"start_time":"2020-08-22T18:02:14.473512","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def to_rgb(grayscale_input, ab_output):\n    color_image = torch.cat((grayscale_input, ab_output), 0).numpy() # combine channels\n    print(color_image.shape)\n    color_image = color_image.transpose((1, 2, 0))  # rescale for matplotlib\n    color_image[:, :, 0:1] = color_image[:, :, 0:1]\n    color_image[:, :, 1:3] = (color_image[:, :, 1:3]) * 128\n    color_image = lab2rgb(color_image.astype(np.float64))\n    grayscale_input = grayscale_input.squeeze().numpy()\n    return color_image","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.013656,"end_time":"2020-08-22T18:02:14.525892","exception":false,"start_time":"2020-08-22T18:02:14.512236","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Given a black and white image with 3 channels(R==G==B), converting it to Lab and giving the L channel as input to predict the ab channels and finally, obtaining the predicted coloured version of the image."},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T18:02:14.561284Z","iopub.status.busy":"2020-08-22T18:02:14.560510Z","iopub.status.idle":"2020-08-22T18:02:14.564453Z","shell.execute_reply":"2020-08-22T18:02:14.564006Z"},"papermill":{"duration":0.024798,"end_time":"2020-08-22T18:02:14.564547","exception":false,"start_time":"2020-08-22T18:02:14.539749","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def prediction(img):\n    a = rgb2lab(img.permute(1, 2, 0))\n    a = torch.tensor(a[:,:,0]).type(torch.FloatTensor)\n    a = a.unsqueeze(0)\n    a = a.unsqueeze(0)\n    xb = to_device(a, device)\n    ab_img = model(xb)\n    xb = xb.squeeze(0)\n    ab_img = ab_img.squeeze(0)\n    return to_rgb(xb.detach().cpu(), ab_img.detach().cpu())","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.013693,"end_time":"2020-08-22T18:02:14.592220","exception":false,"start_time":"2020-08-22T18:02:14.578527","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Testing the model on different black and white images."},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T18:02:14.625258Z","iopub.status.busy":"2020-08-22T18:02:14.624337Z","iopub.status.idle":"2020-08-22T18:02:14.628462Z","shell.execute_reply":"2020-08-22T18:02:14.627964Z"},"papermill":{"duration":0.0221,"end_time":"2020-08-22T18:02:14.628565","exception":false,"start_time":"2020-08-22T18:02:14.606465","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import glob\nfrom PIL import Image\n\nimages = glob.glob(\"../input/test-pic/4yearoldgirl.0.jpg\")","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T18:02:14.661985Z","iopub.status.busy":"2020-08-22T18:02:14.661412Z","iopub.status.idle":"2020-08-22T18:02:14.859047Z","shell.execute_reply":"2020-08-22T18:02:14.858413Z"},"papermill":{"duration":0.216606,"end_time":"2020-08-22T18:02:14.859167","exception":false,"start_time":"2020-08-22T18:02:14.642561","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"img = images[0]\nimage = Image.open(img)\ntrans = T.Compose([T.Resize((256, 256)),T.ToTensor()])\nimg = trans(image)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T18:02:14.920046Z","iopub.status.busy":"2020-08-22T18:02:14.904457Z","iopub.status.idle":"2020-08-22T18:02:15.411200Z","shell.execute_reply":"2020-08-22T18:02:15.411691Z"},"papermill":{"duration":0.53822,"end_time":"2020-08-22T18:02:15.411915","exception":false,"start_time":"2020-08-22T18:02:14.873695","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"f, arr = plt.subplots(1, 2, sharey=True)\narr[0].imshow(img.permute(1, 2, 0))\narr[1].imshow(prediction(img))","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.015471,"end_time":"2020-08-22T18:02:15.443913","exception":false,"start_time":"2020-08-22T18:02:15.428442","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}